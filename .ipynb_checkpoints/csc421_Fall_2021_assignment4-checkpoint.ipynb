{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC421 Fall 2021 Assignment 4 \n",
    "### Author: George Tzanetakis \n",
    "\n",
    "This notebook is based on the topics covered in **Chapter 14 - Probabilistic Reasoning over Time**, **Chapter 20 - Learning probabilistic models**, and **Chapter 19 Learning from Examples** from the book *Artificial Intelligence: A Modern Approach.*  You are welcome and actually it can be educational to look at the code at the aima-code repository as well as other code resources you can find on the web. However, make sure you understand any code that you incoporate. \n",
    "\n",
    "The assignment structure is as follows - each item is worth 1 point: \n",
    "\n",
    "1. Bayesian Network  (Basic) - express network and print CPT  \n",
    "2. Bayesian Network  (Expected) - markdown and direct inference   \n",
    "3. Bayesian Network  (Basic) -  approximate inference (rejection sampling and likelihood weighting) \n",
    "4. Bayesian Netowrk  (Advanced) - naive bayes of movie reviews as bayesian network \n",
    "5. Hidden Markov Models (Basic) - Use HMM to generate plausible DNA sequences and visualize \n",
    "6. Hidden Markov Models (Expected) - Learn HMM from samples for DNA sequences \n",
    "7. Hidden Markov Model (Expected) - Compare classification accuracy of ignoring transition matrix \n",
    "8. Hidden Markov Models (Advanced) - make up HMM scenario for activity detection using 2D coordinates and GMMs  \n",
    "9. Classification (Basic) - Replicate movie review classification using bernoulli Naive Bayes in sklearn \n",
    "10. Classification(Expected) - Explore a standard classification problem with continuous attributes in sklearn \n",
    "\n",
    "The grading will be done in 0.5 increments. 1 point for correct answer, 0.5 points for partial or incorrect \n",
    "but reasonable answer and 0.0 for no answer or completely wrong answer. \n",
    "\n",
    "**Misunderstanding of probability may be the greatest of all impediments\n",
    "to scientific literacy.** \n",
    "\n",
    "**Gould, Stephen Jay** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (Basic)  - 1 point\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"dispnea.png\">\n",
    "\n",
    "Using the convetions for DBNs used in probability.ipynb (from the AIMA authors) encode the diapnea network shown above. Once you have constructed the Bayesian network display the cpt for the Lung Cancer Node (using the API provided not just showing the numbers).\n",
    "\n",
    "The cell below contains the code that defined BayesNodes and BayesNetworks and the following cell \n",
    "shows an example of defining the Burglary network and performing a query using direct enumeration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.21.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.21.4\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement random (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for random\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy \n",
    "!pip install random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "\n",
    "def extend(s, var, val):\n",
    "    \"\"\"Copy dict s and extend it by setting var to val; return copy.\"\"\"\n",
    "    return {**s, var: val}\n",
    "\n",
    "def event_values(event, variables):                                                                      \n",
    "    \"\"\"Return a tuple of the values of variables in event.                                               \n",
    "    >>> event_values ({'A': 10, 'B': 9, 'C': 8}, ['C', 'A'])                                             \n",
    "    (8, 10)                                                                                              \n",
    "    >>> event_values ((1, 2), ['C', 'A'])                                                                \n",
    "    (1, 2)                                                                                               \n",
    "    \"\"\"                                                                                                  \n",
    "    if isinstance(event, tuple) and len(event) == len(variables):                                        \n",
    "        return event                                                                                     \n",
    "    else:                                                                                                \n",
    "        return tuple([event[var] for var in variables])                                                  \n",
    "                      \n",
    "def probability(p):                                                                                      \n",
    "    \"\"\"Return true with probability p.\"\"\"                                                                \n",
    "    return p > random.uniform(0.0, 1.0)  \n",
    "        \n",
    "class ProbDist:\n",
    "    \"\"\"A discrete probability distribution. You name the random variable\n",
    "    in the constructor, then assign and query probability of values.\n",
    "    >>> P = ProbDist('Flip'); P['H'], P['T'] = 0.25, 0.75; P['H']\n",
    "    0.25\n",
    "    >>> P = ProbDist('X', {'lo': 125, 'med': 375, 'hi': 500})\n",
    "    >>> P['lo'], P['med'], P['hi']\n",
    "    (0.125, 0.375, 0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, var_name='?', freq=None):\n",
    "        \"\"\"If freq is given, it is a dictionary of values - frequency pairs,\n",
    "        then ProbDist is normalized.\"\"\"\n",
    "        self.prob = {}\n",
    "        self.var_name = var_name\n",
    "        self.values = []\n",
    "        if freq:\n",
    "            for (v, p) in freq.items():\n",
    "                self[v] = p\n",
    "            self.normalize()\n",
    "\n",
    "    def __getitem__(self, val):\n",
    "        \"\"\"Given a value, return P(value).\"\"\"\n",
    "        try:\n",
    "            return self.prob[val]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    def __setitem__(self, val, p):\n",
    "        \"\"\"Set P(val) = p.\"\"\"\n",
    "        if val not in self.values:\n",
    "            self.values.append(val)\n",
    "        self.prob[val] = p\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Make sure the probabilities of all values sum to 1.\n",
    "        Returns the normalized distribution.\n",
    "        Raises a ZeroDivisionError if the sum of the values is 0.\"\"\"\n",
    "        total = sum(self.prob.values())\n",
    "        if not np.isclose(total, 1.0):\n",
    "            for val in self.prob:\n",
    "                self.prob[val] /= total\n",
    "        return self\n",
    "\n",
    "    def show_approx(self, numfmt='{:.3g}'):\n",
    "        \"\"\"Show the probabilities rounded and sorted by key, for the\n",
    "        sake of portable doctests.\"\"\"\n",
    "        return ', '.join([('{}: ' + numfmt).format(v, p) for (v, p) in sorted(self.prob.items())])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"P({})\".format(self.var_name)\n",
    "\n",
    "\n",
    "class BayesNode:\n",
    "    \"\"\"A conditional probability distribution for a boolean variable,\n",
    "    P(X | parents). Part of a BayesNet.\"\"\"\n",
    "\n",
    "    def __init__(self, X, parents, cpt):\n",
    "        \"\"\"X is a variable name, and parents a sequence of variable\n",
    "        names or a space-separated string. cpt, the conditional\n",
    "        probability table, takes one of these forms:\n",
    "\n",
    "        * A number, the unconditional probability P(X=true). You can\n",
    "          use this form when there are no parents.\n",
    "\n",
    "        * A dict {v: p, ...}, the conditional probability distribution\n",
    "          P(X=true | parent=v) = p. When there's just one parent.\n",
    "\n",
    "        * A dict {(v1, v2, ...): p, ...}, the distribution P(X=true |\n",
    "          parent1=v1, parent2=v2, ...) = p. Each key must have as many\n",
    "          values as there are parents. You can use this form always;\n",
    "          the first two are just conveniences.\n",
    "\n",
    "        In all cases the probability of X being false is left implicit,\n",
    "        since it follows from P(X=true).\n",
    "\n",
    "        >>> X = BayesNode('X', '', 0.2)\n",
    "        >>> Y = BayesNode('Y', 'P', {T: 0.2, F: 0.7})\n",
    "        >>> Z = BayesNode('Z', 'P Q',\n",
    "        ...    {(T, T): 0.2, (T, F): 0.3, (F, T): 0.5, (F, F): 0.7})\n",
    "        \"\"\"\n",
    "        if isinstance(parents, str):\n",
    "            parents = parents.split()\n",
    "\n",
    "        # We store the table always in the third form above.\n",
    "        if isinstance(cpt, (float, int)):  # no parents, 0-tuple\n",
    "            cpt = {(): cpt}\n",
    "        elif isinstance(cpt, dict):\n",
    "            # one parent, 1-tuple\n",
    "            if cpt and isinstance(list(cpt.keys())[0], bool):\n",
    "                cpt = {(v,): p for v, p in cpt.items()}\n",
    "\n",
    "        assert isinstance(cpt, dict)\n",
    "        for vs, p in cpt.items():\n",
    "            assert isinstance(vs, tuple) and len(vs) == len(parents)\n",
    "            assert all(isinstance(v, bool) for v in vs)\n",
    "            assert 0 <= p <= 1\n",
    "\n",
    "        self.variable = X\n",
    "        self.parents = parents\n",
    "        self.cpt = cpt\n",
    "        self.children = []\n",
    "\n",
    "    def p(self, value, event):\n",
    "        \"\"\"Return the conditional probability\n",
    "        P(X=value | parents=parent_values), where parent_values\n",
    "        are the values of parents in event. (event must assign each\n",
    "        parent a value.)\n",
    "        >>> bn = BayesNode('X', 'Burglary', {T: 0.2, F: 0.625})\n",
    "        >>> bn.p(False, {'Burglary': False, 'Earthquake': True})\n",
    "        0.375\"\"\"\n",
    "        assert isinstance(value, bool)\n",
    "        ptrue = self.cpt[event_values(event, self.parents)]\n",
    "        return ptrue if value else 1 - ptrue\n",
    "\n",
    "    def sample(self, event):\n",
    "        \"\"\"Sample from the distribution for this variable conditioned\n",
    "        on event's values for parent_variables. That is, return True/False\n",
    "        at random according with the conditional probability given the\n",
    "        parents.\"\"\"\n",
    "        return probability(self.p(True, event))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.variable, ' '.join(self.parents)))\n",
    "    \n",
    "    \n",
    "class BayesNet:\n",
    "    \"\"\"Bayesian network containing only boolean-variable nodes.\"\"\"\n",
    "\n",
    "    def __init__(self, node_specs=None):\n",
    "        \"\"\"Nodes must be ordered with parents before children.\"\"\"\n",
    "        self.nodes = []\n",
    "        self.variables = []\n",
    "        node_specs = node_specs or []\n",
    "        for node_spec in node_specs:\n",
    "            self.add(node_spec)\n",
    "\n",
    "    def add(self, node_spec):\n",
    "        \"\"\"Add a node to the net. Its parents must already be in the\n",
    "        net, and its variable must not.\"\"\"\n",
    "        node = BayesNode(*node_spec)\n",
    "        assert node.variable not in self.variables\n",
    "        assert all((parent in self.variables) for parent in node.parents)\n",
    "        self.nodes.append(node)\n",
    "        self.variables.append(node.variable)\n",
    "        for parent in node.parents:\n",
    "            self.variable_node(parent).children.append(node)\n",
    "\n",
    "    def variable_node(self, var):\n",
    "        \"\"\"Return the node for the variable named var.\n",
    "        >>> burglary.variable_node('Burglary').variable\n",
    "        'Burglary'\"\"\"\n",
    "        for n in self.nodes:\n",
    "            if n.variable == var:\n",
    "                return n\n",
    "        raise Exception(\"No such variable: {}\".format(var))\n",
    "\n",
    "    def variable_values(self, var):\n",
    "        \"\"\"Return the domain of var.\"\"\"\n",
    "        return [True, False]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'BayesNet({0!r})'.format(self.nodes)\n",
    "    \n",
    "    \n",
    "def enumerate_all(variables, e, bn):\n",
    "    \"\"\"Return the sum of those entries in P(variables | e{others})\n",
    "    consistent with e, where P is the joint distribution represented\n",
    "    by bn, and e{others} means e restricted to bn's other variables\n",
    "    (the ones other than variables). Parents must precede children in variables.\"\"\"\n",
    "    if not variables:\n",
    "        return 1.0\n",
    "    Y, rest = variables[0], variables[1:]\n",
    "    Ynode = bn.variable_node(Y)\n",
    "    if Y in e:\n",
    "        return Ynode.p(e[Y], e) * enumerate_all(rest, e, bn)\n",
    "    else:\n",
    "        return sum(Ynode.p(y, e) * enumerate_all(rest, extend(e, Y, y), bn)\n",
    "                   for y in bn.variable_values(Y))\n",
    "\n",
    "def enumeration_ask(X, e, bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.9]\n",
    "    Return the conditional probability distribution of variable X\n",
    "    given evidence e, from BayesNet bn.\n",
    "    >>> enumeration_ask('Burglary', dict(JohnCalls=T, MaryCalls=T), burglary\n",
    "    ...  ).show_approx()\n",
    "    'False: 0.716, True: 0.284'\"\"\"\n",
    "    assert X not in e, \"Query variable must be distinct from evidence\"\n",
    "    Q = ProbDist(X)\n",
    "    for xi in bn.variable_values(X):\n",
    "        Q[xi] = enumerate_all(bn.variables, extend(e, X, xi), bn)\n",
    "    return Q.normalize()\n",
    "\n",
    "def consistent_with(event, evidence):\n",
    "    \"\"\"Is event consistent with the given evidence?\"\"\"\n",
    "    return all(evidence.get(k, v) == v for k, v in event.items())\n",
    "\n",
    "def prior_sample(bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.13]\n",
    "    Randomly sample from bn's full joint distribution.\n",
    "    The result is a {variable: value} dict.\n",
    "    \"\"\"\n",
    "    event = {}\n",
    "    for node in bn.nodes:\n",
    "        event[node.variable] = node.sample(event)\n",
    "    return event\n",
    "\n",
    "def rejection_sampling(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.14]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn, using N samples.\n",
    "    Raises a ZeroDivisionError if all the N samples are rejected,\n",
    "    i.e., inconsistent with e.\n",
    "    >>> random.seed(47)\n",
    "    >>> rejection_sampling('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.7, True: 0.3'\n",
    "    \"\"\"\n",
    "    counts = {x: 0 for x in bn.variable_values(X)}  # bold N in [Figure 14.14]\n",
    "    for j in range(N):\n",
    "        sample = prior_sample(bn)  # boldface x in [Figure 14.14]\n",
    "        if consistent_with(sample, e):\n",
    "            counts[sample[X]] += 1\n",
    "    return ProbDist(X, counts)\n",
    "\n",
    "def weighted_sample(bn, e):\n",
    "    \"\"\"\n",
    "    Sample an event from bn that's consistent with the evidence e;\n",
    "    return the event and its weight, the likelihood that the event\n",
    "    accords to the evidence.\n",
    "    \"\"\"\n",
    "    w = 1\n",
    "    event = dict(e)  # boldface x in [Figure 14.15]\n",
    "    for node in bn.nodes:\n",
    "        Xi = node.variable\n",
    "        if Xi in e:\n",
    "            w *= node.p(e[Xi], event)\n",
    "        else:\n",
    "            event[Xi] = node.sample(event)\n",
    "    return event, w\n",
    "\n",
    "def likelihood_weighting(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.15]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn.\n",
    "    >>> random.seed(1017)\n",
    "    >>> likelihood_weighting('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.702, True: 0.298'\n",
    "    \"\"\"\n",
    "    W = {x: 0 for x in bn.variable_values(X)}\n",
    "    for j in range(N):\n",
    "        sample, weight = weighted_sample(bn, e)  # boldface x, w in [Figure 14.15]\n",
    "        W[sample[X]] += weight\n",
    "    return ProbDist(X, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}\n",
      "0.2841718353643929 0.7158281646356071\n",
      "0.15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'False: 0.604, True: 0.396'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   burglary = BayesNet([\n",
    "        ('Burglary', '', 0.001),\n",
    "        ('Earthquake', '', 0.002),\n",
    "        ('Alarm', 'Burglary Earthquake',\n",
    "         {(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}),\n",
    "        ('JohnCalls', 'Alarm', {True: 0.90, False: 0.05}),\n",
    "        ('MaryCalls', 'Alarm', {True: 0.70, False: 0.01})\n",
    "    ])\n",
    "    \n",
    "print(burglary.variable_node('Alarm').cpt)\n",
    "ans_dist = enumeration_ask('Burglary', {'JohnCalls': True, 'MaryCalls': True}, burglary)\n",
    "print(ans_dist[True],ans_dist[False])\n",
    "\n",
    "\n",
    "p = rejection_sampling('Burglary', dict(JohnCalls=True, MaryCalls=True), burglary, 10000)\n",
    "print(p[True])\n",
    "\n",
    "likelihood_weighting('Burglary', dict(JohnCalls=True, MaryCalls=True),burglary, 10000).show_approx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True,): 0.1, (False,): 0.01}\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "#Using the convetions for DBNs used in \n",
    "#probability.ipynb (from the AIMA authors) encode the diapnea network shown above. \n",
    "\n",
    "\n",
    "\n",
    "DiapneaNetwork = BayesNet([\n",
    "        ('VisitAsia', '', 0.01),\n",
    "        ('Smoker', '', 0.5),\n",
    "        ('Tuberculosis', 'VisitAsia', {True: 0.05, False: 0.01}),\n",
    "        ('LungCancer', 'Smoker', {True: 0.1, False: 0.01}),\n",
    "        ('Bronchitis', 'Smoker', {True: 0.6, False: 0.3}),\n",
    "        ('EitherTBorLC', 'LungCancer Tuberculosis',\n",
    "         {(True, True): 1, (True, False): 1, (False, True): 1, (False, False): 0}),\n",
    "        ('PositiveXRay', 'EitherTBorLC', {True: 0.98, False: 0.05}),\n",
    "        ('Dispnea', 'EitherTBorLC Bronchitis',\n",
    "         {(True, True): 0.9, (True, False): 0.7, (False, True): 0.8, (False, False): 0.1}),\n",
    "    ])\n",
    "    \n",
    "#Display the cpt for the Lung Cancer Node \n",
    "print(DiapneaNetwork.variable_node('LungCancer').cpt)\n",
    "\n",
    "#The cell below contains the code that defined BayesNodes and BayesNetworks and the \n",
    "#following cell shows an example of defining the Burglary network and performing a query using direct enumeration.\n",
    "#ans_dist = enumeration_ask('VisitAsia', {'Tuberculosis': True, 'Bronchitis': True,'LungCancer': True,}, DiapneaNetwork)\n",
    "#print(ans_dist[True],ans_dist[False])\n",
    "#p = rejection_sampling('VisitAsia', dict(Tuberculosis=True, Bronchitis=True, LungCancer=True), DiapneaNetwork, 10000)\n",
    "#print(p[True])\n",
    "#likelihood_weighting('VisitAsia', dict(Tuberculosis=True, Bronchitis=True, LungCancer=True),DiapneaNetwork, 10000).show_approx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (Expected) 1 point \n",
    "\n",
    "Answer using exact inference with enumeration the following query: given that a patient has been in Asia and has a positive xray, what is the likelihood of having dispnea?\n",
    "\n",
    "Write down using markdown the expression that corresponds to this query and the corresponding numbers from the CPT. There will be multiple sums and subscripts. Calculate the result using a calculator.\n",
    "\n",
    "Write code for the same query using enumeration_ask and confirm that the result is the same for the same query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Q2 markdown: \n",
    " \n",
    "$P(D | a, x)$ \n",
    "\n",
    " = $\\alpha \\sum_s \\sum_t \\sum_l \\sum_b \\sum_e P(a,s,t,l,b,e,x,D) $\n",
    "\n",
    "rewrite as product \n",
    "\n",
    "= $\\alpha \\sum_s \\sum_t \\sum_l \\sum_b \\sum_e P(+a) P(s) P(t|+a) P(l|s) P(b|s) P(e|l,t) P(+x|e) P(D|e,b)$\n",
    "\n",
    "= 0.6811011940658546\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6811011940658546\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "#given that a patient has been in Asia and has a positive xray, what is the likelihood of having dispnea?\n",
    "\n",
    "#in markdown write the expression that corresponds to this query and \n",
    "# the corresponding numbers from the CPT \n",
    "#calculate the result (using calculator)\n",
    "#write code for the same query using enumeration_ask and confirm that the result is the same for the same query \n",
    "\n",
    "ans_dist = enumeration_ask('Dispnea', {'VisitAsia': True, 'PositiveXRay': True}, DiapneaNetwork)\n",
    "print(ans_dist[True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 3 (Basic) - 1 point\n",
    "\n",
    "Answer using approximate inference the same query using both rejection sampling and likelihood weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n",
      "0.6908651625986373\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "#answer using approximate inference the same query using both rejection sampling and likelihood weighting \n",
    "\n",
    "\n",
    "#rejection sampling \n",
    "\n",
    "p = rejection_sampling('Dispnea', dict(VisitAsia=True, PositiveXRay=True), DiapneaNetwork, 10000)\n",
    "print(p[True])\n",
    "\n",
    "\n",
    "#likelihood weighting \n",
    "p1 = likelihood_weighting('Dispnea', dict(VisitAsia=True, PositiveXRay= True), DiapneaNetwork, 10000)\n",
    "print(p1[True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 4 (ADVANCED) - 1 point \n",
    "\n",
    "A Naive Bayes classifier can be considered as a Bayesian Network. The classification problem can then be expressed as setting all the variables corresponding to the features as evidence and querying the probability for the class. Express the Bernoulli Naive Bayes classifier you implemented in the previous assignment as a Bayesian Network using the probability.ipynb conventions used in this notebook. Now that you have a DBN express and solve the classification problem as a query and go over all the previous steps for this particular problem. More specifically do exact inference by enumeration, exact inference by variable elimination, approximate inference by rejection sampling and approximate inference by likelihood weighting to answer the query and show the results. Use 4 specific examples (2 positive and 2 negative) from the training dataset to show how the prediction using the Bayesian network works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "#exact inference by direct enumeration \n",
    "#and approximate inference using rejection sampling with the code provided.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (Basic) -1 point\n",
    "\n",
    "\n",
    "The next three question explore hidden markov models (HMMs) and use the hmmlearn Python library. You can use the code for the weather example in the probabilistic reasoning over time notebook we covered in class as a template for writing your code. \n",
    "\n",
    "The problem used in inspired by the use of HMMs in bioinformatics. \n",
    "There are several simplifications made to make it reasonable as part of an assignment. DNA sequences can be considered strings over an alphabet of 4 symbols/nucleobases **A,C,T,G (adenine, cytosine, thymine, guanine**. Parts of a DNA sequence are dense with C and G and other parts are sparse with C and G and it is of interest to biologists to identify these regions. \n",
    "\n",
    "We will model the CG-dense **(CGD)** and **CG-sparse** (CGS) as hidden states and the nucleobases are the observations. Through experimental data we have the following information: \n",
    "\n",
    "1. The transition probability from CGR to CGP is 0.37 and the probability of staying in CGR is 0.63. The transition probability from CGP to CGR is similarly 0.37 with 0.63 being the probability of staying in CGP. \n",
    "\n",
    "2. The observation probabilities of CGR regions are: A: 0.15, C:0.35, G: 0.35, and T:0.15. The observation probabilities of CGP regions are: A: 0.40, C: 0.10, G: 0.10, T: 0.40 \n",
    "\n",
    "3. You can assume that the initial state probabilities are the same (0.5) \n",
    "\n",
    "4. For visualization of the DNA sequences use the following color mapping: A: red, C: green, T: blue, G: yellow, and for CGD: black \n",
    "and CGR: white \n",
    "\n",
    "\n",
    "Define this HMM model using the **hmmlearn** conventions. Then use the created model to generate a sequence of 1000 samples (i.e both hidden states and corresponding observations). Use the colors above \n",
    "to visualize the sequence of samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hmmlearn in /opt/conda/lib/python3.9/site-packages (0.2.6)\n",
      "Requirement already satisfied: numpy>=1.10 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.21.4)\n",
      "Requirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (3.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.21.4)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.2 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.3.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Collecting setuptools-scm>=4\n",
      "  Downloading setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-8.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.2-py3-none-any.whl (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.21.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib) (49.6.0.post20210108)\n",
      "Collecting tomli>=1.0.0\n",
      "  Downloading tomli-1.2.2-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: tomli, setuptools-scm, pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.28.2 kiwisolver-1.3.2 matplotlib-3.5.0 pillow-8.4.0 setuptools-scm-6.3.2 tomli-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install hmmlearn\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "transmat = np.array([[0.37, 0.63], \n",
    "                    [0.37, 0.63]])\n",
    "\n",
    "start_prob = np.array([0.5, 0.5])\n",
    "\n",
    "emission_probs = np.array([[0.15, 0.35, 0.35, 0.15], \n",
    "                           [0.4, 0.1, 0.1, 0.4]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_ = start_prob \n",
    "model.transmat_ = transmat \n",
    "model.emissionprob_ = emission_probs\n",
    "\n",
    "# sample the model - X is the observed values \n",
    "# and Z is the \"hidden\" states \n",
    "X, Z = model.sample(1000)\n",
    "\n",
    "# we have to re-define state2color and obj2color as the hmm-learn \n",
    "# package just outputs numbers for the states \n",
    "state2color = {} \n",
    "state2color[0] = 'black'\n",
    "state2color[1] = 'white'\n",
    "plot_weather_samples(Z, state2color, 'states')\n",
    "\n",
    "samples = [item for sublist in X for item in sublist]\n",
    "obj2color = {} \n",
    "obj2color[0] = 'red'\n",
    "obj2color[1] = 'green'\n",
    "obj2color[2] = 'blue'\n",
    "obj2color[3] = 'yellow'\n",
    "plot_weather_samples(samples, obj2color, 'observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6 (Expected) -1 point\n",
    "\n",
    "Generate 10000 samples using the defined hmm for generating DNA sequences. Learn the HMM in an unsupervised fashion similarly to what we did with the weather example i.e only use the observation samples not the \"hidden\" states. Constrast the original HMM to the HMM estimated from the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix\n",
      "Estimated model:\n",
      "[[0.53412639 0.46587361]\n",
      " [0.53830369 0.46169631]]\n",
      "Original model:\n",
      "[[0.37 0.63]\n",
      " [0.37 0.63]]\n",
      "Emission probabilities\n",
      "Estimated model\n",
      "[[0.41799018 0.20773453 0.15610217 0.21817311]\n",
      " [0.18007425 0.16046063 0.2472636  0.41220151]]\n",
      "Original model\n",
      "[[0.15 0.35 0.35 0.15]\n",
      " [0.4  0.1  0.1  0.4 ]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "#generate 10000 samples using the defined hmm for generating DNA sequences\n",
    "\n",
    "#learn the HMM in an unsupervised fashion similarily to hat we did with the weather exmaple \n",
    "    #ie only use the observation samples not the hidden states\n",
    "    \n",
    "#contrast the origional HMM to the HMM estimated from the data\n",
    "\n",
    "#generate 10,000 samples using the defined hmm \n",
    "\n",
    "#only use observed samples, not hidden states\n",
    "\n",
    "# generate the samples \n",
    "X, Z = model.sample(10000)\n",
    "# learn a new model \n",
    "estimated_model = hmm.MultinomialHMM(n_components=2, n_iter=10000).fit(X)\n",
    "\n",
    "print(\"Transition matrix\")\n",
    "print(\"Estimated model:\")\n",
    "print(estimated_model.transmat_)\n",
    "print(\"Original model:\")\n",
    "print(model.transmat_)\n",
    "\n",
    "print(\"Emission probabilities\")\n",
    "print(\"Estimated model\")\n",
    "print(estimated_model.emissionprob_)\n",
    "print(\"Original model\")\n",
    "print(model.emissionprob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7 (Expected) -1 point\n",
    "\n",
    "Write a function called **classification_accuracy** that takes as input \n",
    "two arrays or lists of states and returns the number of states that are the same in both lists as a percentage. \n",
    "\n",
    "Consider the original sequences of states of the generated samples \n",
    "as ground truth. Then use the estimated model from the previous \n",
    "question to generate predicted states from the observation samples. \n",
    "That is the maximum likelihood sequence estimation problem. \n",
    "Note that the predicted states might be inverted compared to the original and you need to deal with that in your code (see the class notebook for details). Now compute the accuracy between the predicted \n",
    "sequence of states and the ground truth sequence of states. \n",
    "This is similar to the visual comparison of the original and predicted states in the provided notebook but using a quantified \n",
    "metric rather than a visualization. \n",
    "\n",
    "Now replace the transition model of the original HMM with a transition model that is all 0.5 i.e there is no transition information. Effectively this disregards any temporal dependenices and each time step is decided independently. In fact it corresponds to a Naive Bayes classifier with a single feature which is the nucleobase observation. \n",
    "\n",
    "What is the classification accuracy in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sequence:  0.5042\n",
      "No transition information:  0.5133\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "    \n",
    "#print(X)  \n",
    "#print(Z)\n",
    "\n",
    "def classification_accuracy(X,Z):\n",
    "\n",
    "    score = 0\n",
    "    X = [item for sublist in X for item in sublist]  \n",
    "    for x,z in zip(X,Z):\n",
    "        if x%2 == 0 and z == 0:   #  EVEN AND Z == 0\n",
    "            score += 1 \n",
    "#note that the predicted states might be inverted compared to the orignal and you need to deal with that in your code \n",
    "        if x%2 == 1 and z == 1:   #  ODD AND Z == 1 \n",
    "\n",
    "            score += 1 \n",
    "    return score/len(Z)\n",
    "\n",
    "#consider the origional sequence of states from the observation samples\n",
    "#then compute the accuracy between the predicted sequence of states and the ground truth sequence of states \n",
    "\n",
    "transmat = np.array([[0.37, 0.63], \n",
    "                    [0.37, 0.63]])\n",
    "\n",
    "start_prob = np.array([0.5, 0.5])\n",
    "\n",
    "emission_probs = np.array([[0.15, 0.35, 0.35, 0.15], \n",
    "                           [0.4, 0.1, 0.1, 0.4]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_ = start_prob \n",
    "model.transmat_ = transmat \n",
    "model.emissionprob_ = emission_probs\n",
    "X, Z = model.sample(10000)\n",
    "\n",
    "print('Original sequence: ',classification_accuracy(X,Z))\n",
    "\n",
    "\n",
    "##########################################\n",
    "\n",
    "#then replace the transition model of the HMM with a transition model that is all 0.5 ie. there is no transition inormation \n",
    "#this disregards any temporal dependencies and each time step is decided independently \n",
    "#it corresponds to a naive abyes classifier with a single feature which is the nucleobase observation \n",
    "\n",
    "transmat = np.array([[0.5, 0.5], \n",
    "                    [0.5, 0.5]])\n",
    "\n",
    "start_prob = np.array([0.5, 0.5])\n",
    "\n",
    "emission_probs = np.array([[0.15, 0.35, 0.35, 0.15], \n",
    "                           [0.4, 0.1, 0.1, 0.4]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_ = start_prob \n",
    "model.transmat_ = transmat \n",
    "model.emissionprob_ = emission_probs\n",
    "X, Z = model.sample(10000)\n",
    "\n",
    "print('No transition information: ',classification_accuracy(X,Z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8 (Advanced) -1 point\n",
    "\n",
    "This question is a bit more open ended, will require some creativity and extra work. Consider the following problem: during your day your cell phone collects location data in terms of x,y coordinates. You do different activities such as going to university, eating, going to the gym. These activities take place in particular locations such as Restaurant A and Restaurant B or Gym A, Gym B and each particular location can be thought of as a two-dimensional Gaussian distribution of location points. If you consider the activity as the hidden state and the location as the observation you have a Hidden Markov Model. Because activities take place in multiple locations you can model this as a Gaussian Mixture Model (GMM). Each Gaussian will be multivariate 2D Gaussian distribution characterized by two means and and a 2 by 2 covariance matrix.\n",
    "\n",
    "Consider a hypothetical scenario with 3 activities (eat, study, exercise) and 3 locations (GMM components) for each activity. You will need to do some reading about how GMMs work. You can come up \n",
    "with reasonable estimates for the associated parameters. \n",
    "\n",
    "Basically the goal is the follow the format of the Markov Chain and HMM notebook and create appropriate visualizations using this problem.\n",
    "\n",
    "Visualize on a 2D plane using circles the different locations and corresponding mixture components\n",
    "Generate a dataset using a Hidden Markov Model of the problem\n",
    "Visualize the dataset on a 2D plane\n",
    "Show how you can learn the parameters of this HMM using https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn.hmm.GMMHMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9 (Basic) - 1 point\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this question is to get some familiarity with scikit-learn: https://scikit-learn.org/stable/\n",
    "\n",
    "Replicate movie review classification from the previous assignment using bernoulli Naive Bayes in sklearn. This is relatively straightforward you simply need to create appropriate binary feature matrix and labels. Report on the classification accuracy and confusion matrix for that problem using 3-fold cross-validation. \n",
    "You will need to consult the execllent sklearn documentation for details. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.21.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.21.4\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Using cached scipy-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.4)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.1 scipy-1.7.3 sklearn-0.0 threadpoolctl-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy \n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy and confusion matrix using 3-fold cross-validation\n",
      "Returns [1] if classified as negative, Returns [0] if classified as postive\n",
      "\n",
      "\n",
      "negative review: cv231_11028.txt: [1, 1, 1, 0, 1, 1, 0, 0]\n",
      "[1] --> should be [1]\n",
      "correct\n",
      "\n",
      "\n",
      " positive review: cv706_25883.txt: [0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1] --> should be [0]\n",
      "incorrect\n",
      "\n",
      "\n",
      "positive review: cv042_10982.txt: [0, 1, 0, 0, 0, 0, 1, 0]\n",
      "[1] --> should be [0]\n",
      "incorrect\n",
      "\n",
      "\n",
      "CONFUSION MATRIX for each fold\n",
      "[[239  83]\n",
      " [126 219]]\n",
      "[[249  84]\n",
      " [138 196]]\n",
      "[[265  80]\n",
      " [141 180]]\n",
      "\n",
      "\n",
      "3-FOLD CROSS VAL\n",
      "[0.65967016 0.70014993 0.66666667]\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE GOES HERE # \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "features = ['awful', 'bad', 'boring', 'dull', 'effective', 'enjoyable', 'great', 'hilarious']\n",
    "\n",
    "######## Read in Reviews ########\n",
    "\n",
    "reviews = []\n",
    "              \n",
    "for s, d, files in os.walk('/home/jovyan/assignment-4/txt_sentoken/pos'):  #read in pos review files\n",
    "    for file in files:\n",
    "        with open(s + os.sep + file) as f:\n",
    "            #readpos = f.read().lower()\n",
    "            reviews.append(f.read())\n",
    "\n",
    "for s, d, files in os.walk('/home/jovyan/assignment-4/txt_sentoken/neg'):  #read in neg review files\n",
    "    for file in files:\n",
    "        with open(s + os.sep + file) as f:\n",
    "            #readneg = f.read().lower()\n",
    "            reviews.append(f.read())\n",
    "\n",
    "            \n",
    "######## Count Vectorize to create Feature Matrix ############\n",
    "\n",
    "cv = CountVectorizer(vocabulary = features, binary = True)\n",
    "cv_fit=cv.fit_transform(reviews)\n",
    "feature_matrix = cv_fit.toarray()\n",
    "X = feature_matrix \n",
    "\n",
    "\n",
    "######## Y = 0 == label for pos review 1 == label for neg review  ############\n",
    "Y1 = [0]*1000\n",
    "Y2 = [1]*1000\n",
    "Y = Y1 + Y2\n",
    "\n",
    "\n",
    "######## Review Classification Prediction using BernoulliNB ########\n",
    "#Review 1\n",
    "\n",
    "R1 = [1, 1, 1, 0, 1, 1, 0, 0]\n",
    "R1 = np.array(R1)\n",
    "R1 = R1.reshape(1,-1)\n",
    "#Review 2 \n",
    "\n",
    "R2 =  [0, 1, 0, 0, 0, 0, 0, 0]\n",
    "R2 = np.array(R1)\n",
    "R2 = R1.reshape(1,-1)\n",
    "#Review 3\n",
    "\n",
    "R3 = [0, 1, 0, 0, 0, 0, 1, 0]\n",
    "R3 = np.array(R1)\n",
    "R3 = R1.reshape(1,-1)\n",
    "\n",
    "\n",
    "####   BernoulliNB()  ###\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "print('Classification accuracy and confusion matrix using 3-fold cross-validation')\n",
    "\n",
    "print('Returns [1] if classified as negative, Returns [0] if classified as postive')\n",
    "print('\\n')\n",
    "print('negative review: cv231_11028.txt: [1, 1, 1, 0, 1, 1, 0, 0]')\n",
    "print(clf.predict(R1), '--> should be [1]')\n",
    "\n",
    "if (clf.predict(R1) == [1]):\n",
    "    print('correct')\n",
    "else:\n",
    "    print('incorrect')\n",
    "print('\\n')\n",
    "print(' positive review: cv706_25883.txt: [0, 1, 0, 0, 0, 0, 0, 0]')\n",
    "print(clf.predict(R2), '--> should be [0]')\n",
    "\n",
    "if (clf.predict(R2) == [0]):\n",
    "    print('correct')\n",
    "else:\n",
    "    print('incorrect')\n",
    "print('\\n')\n",
    "print('positive review: cv042_10982.txt: [0, 1, 0, 0, 0, 0, 1, 0]')\n",
    "print(clf.predict(R3), '--> should be [0]')\n",
    "\n",
    "if (clf.predict(R3) == [0]):\n",
    "    print('correct')\n",
    "else:\n",
    "    print('incorrect')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "print('CONFUSION MATRIX for each fold')\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "kf = KFold(n_splits=3,shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_predict = clf.predict(X_test)\n",
    "    conf_mat = confusion_matrix(Y_test, Y_predict) #, labels = [0,1])\n",
    "    print(conf_mat)\n",
    "\n",
    "\n",
    "print('\\n') \n",
    "print('3-FOLD CROSS VAL')\n",
    "k_fold = KFold(n_splits=3, shuffle=True)\n",
    "print(cross_val_score(clf, X, Y, cv=k_fold, n_jobs=1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10 (Expected) - 1 point \n",
    "\n",
    "The goal of this question is to give you some familiarity with having continuous features and comparing different classifiers. \n",
    "\n",
    "\n",
    "The goal of this question is to give you some familiarity with having continuous features and comparing different classifiers. For this question use the breast cancer dataset from sklearn: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancerTrain and compare three classifiers using this dataset using 3-fold \n",
    "cross-validation to calculate the classification accuracy and classification report: \n",
    "1. The Gaussian Naive Bayes classifier (with default parameters) \n",
    "(from sklearn.naive_bayes import GaussianNB) \n",
    "2. Linear support vector machine (with default parameters) \n",
    "(from sklearn.svm import LinearSVC) \n",
    "3. Decision tree (with default parameters) \n",
    "(from sklearn.tree import DecisionTreeClassifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes : [0.92957746 0.92957746 0.93661972]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.27      0.15      0.19        55\n",
      "      benign       0.58      0.75      0.66        88\n",
      "\n",
      "    accuracy                           0.52       143\n",
      "   macro avg       0.43      0.45      0.42       143\n",
      "weighted avg       0.46      0.52      0.48       143\n",
      "\n",
      "[[ 8 47]\n",
      " [22 66]]\n",
      "Linear Vector Machine : [0.91549296 0.76760563 0.93661972]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.38      1.00      0.56        55\n",
      "      benign       0.00      0.00      0.00        88\n",
      "\n",
      "    accuracy                           0.38       143\n",
      "   macro avg       0.19      0.50      0.28       143\n",
      "weighted avg       0.15      0.38      0.21       143\n",
      "\n",
      "[[55  0]\n",
      " [88  0]]\n",
      "Decision Tree : [0.9084507  0.90140845 0.91549296]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.33      0.35      0.34        55\n",
      "      benign       0.58      0.57      0.57        88\n",
      "\n",
      "    accuracy                           0.48       143\n",
      "   macro avg       0.46      0.46      0.46       143\n",
      "weighted avg       0.49      0.48      0.48       143\n",
      "\n",
      "[[19 36]\n",
      " [38 50]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#compare three classifiers using the dataset using 3-fold cross validating \n",
    "#--> to calculate the clasification accuracy and classification report\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y) \n",
    "\n",
    "cv = KFold(n_splits=3, random_state=1, shuffle=True)\n",
    "\n",
    "models = [GaussianNB(), LinearSVC(), DecisionTreeClassifier()]\n",
    "names = [\"Naive Bayes\",\"Linear Vector Machine\",  \"Decision Tree\"]\n",
    "\n",
    "for model, name in zip(models, names):\n",
    "    cvs = cross_val_score(model,X_train,Y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print(name, ':', cvs)\n",
    "    \n",
    "    classifier_tree = model\n",
    "    y_predict = classifier_tree.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, y_predict, target_names=class_names))\n",
    "    print(confusion_matrix(y_test, y_predict))\n",
    "\n",
    "#print(Y_train.shape)\n",
    "#print(Y_predict.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data = load_breast_cancer()\n",
    "#X = data.data\n",
    "#Y = data.target\n",
    "\n",
    "#class_names = data.target_names\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "#classifier_tree = DecisionTreeClassifier()\n",
    "\n",
    "#y_predict = classifier_tree.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "\n",
    "#print(classification_report(y_test, y_predict, target_names=class_names))\n",
    "\n",
    "#print(confusion_matrix(y_test, y_predict))\n",
    "\n",
    "#clf = BernoulliNB()\n",
    "#clf.fit(X_train, Y_train)\n",
    "\n",
    "#X_pred = clf.predict(X_train)\n",
    "\n",
    "#print(classification_report(X_train,X_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
